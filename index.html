<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ArabicNLP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons/css/academicons.min.css">

	</head>
	<body class="homepage is-preload">
		<div id="page-wrapper">


<!-- Header -->
<header id="header">
    <div class="logo container">
        <div>
            <h1><a href="index.html" id="logo">ArabicNLP.uk</a></h1>
            <p>Open-source datasets, corpora, and tools for Arabic NLP</p>
        </div>
    </div>
</header>


			<!-- Nav -->
			<nav id="nav">
				<ul>
					<li class="current"><a href="index.html">Home</a></li>

					<li><a href="https://github.com/drelhaj" target="_blank">GitHub</a></li>
					<li><a href="https://huggingface.co/drelhaj" target="_blank">HuggingFace</a></li>
					<li><a href="https://vinnlp.com/" target="_blank">NLP @ VinUni</a></li>
					<li><a href="https://ucrel.lancs.ac.uk/" target="_blank">UCREL</a></li>
				</ul>
			</nav>


<!-- Banner -->
<section id="banner">
    <div class="content">

        <img src="images/logo.png" alt="ArabicNLP Logo" style="max-width: 200px; margin-bottom: 20px;">

        <h2>Welcome to ArabicNLP.uk</h2>
        <p>Home to open-source Arabic NLP datasets, corpora, and tools — created and maintained by 
           <a href="https://elhaj.uk" target="_blank">Dr Mo El-Haj</a>.
        </p>
        <a href="#main" class="button scrolly">Explore Resources</a>
    </div>
</section>




			<!-- Main -->
				<section id="main">
					<div class="container">
						<div class="row gtr-200">
							<div class="col-12">

								<!-- Highlight -->
<br>
	<!-- Highlight -->
	<section class="box highlight">
			<header>
			<h2>Key NLP Resources & Collaborations</h2>
			<p>Your main access points for datasets, models, and long-term collaborations</p>
		</header>

		<p>
			Explore the core sources powering Arabic NLP research — models, datasets, repositories,  
			and institutional collaborations.
		</p>
		
<ul class="special resources-icons">

    <li>
        <a href="https://github.com/orgs/ArabicNLP-UK/" target="_blank" class="resource-link">
            <img src="images/logo.png" alt="ArabicNLP-UK" class="resource-icon">
            <span class="resource-label">ArabicNLP<br>GitHub</span>
        </a>
    </li>

    <li>
        <a href="https://github.com/UCREL" target="_blank" class="resource-link">
            <img src="images/ucrel.png" alt="UCREL" class="resource-icon">
            <span class="resource-label">UCREL<br>GitHub</span>
        </a>
    </li>

    <li>
        <a href="https://huggingface.co/drelhaj" target="_blank" class="resource-link">
            <img src="images/hf-logo.png" alt="HuggingFace" class="resource-icon">
            <span class="resource-label">Mo El-Haj<br>HuggingFace</span>
        </a>
    </li>

    <li>
        <a href="https://github.com/drelhaj" target="_blank" class="resource-link">
            <img src="images/github.svg" alt="GitHub" class="resource-icon">
            <span class="resource-label">Mo El-Haj<br>GitHub</span>
        </a>
    </li>

    <li>
        <a href="https://github.com/VinNLP" target="_blank" class="resource-link">
            <img src="images/vinnlpl.png" alt="NLP @ VinUni" class="resource-icon">
            <span class="resource-label">NLP @ VinUni<br>GitHub</span>
        </a>
    </li>

</ul>
	</section>

	<div class="col-12">

    <!-- Features -->
    <section class="box features">
        <h2 class="major"><span>Featured Arabic NLP Resources</span></h2>
        <div>
            <div class="row">

                <!-- AraFinNews -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/AraFinNews" target="_blank" class="image featured">
                            <img src="images/arafinnews.jpg" alt="AraFinNews" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/AraFinNews" target="_blank">AraFinNews</a></h3>
                        <p>
                            212,500 financial news articles from Argaam.com for summarisation, event extraction, and financial NLP.
                        </p>
                    </section>
                </div>

                <!-- ArabJobs -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/ArabJobs" target="_blank" class="image featured">
                            <img src="images/arabjobs.jpg" alt="ArabJobs" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/ArabJobs" target="_blank">ArabJobs</a></h3>
                        <p>
                            8,546 job ads with metadata for gender, salary, profession, and dialect-sensitive labour market analysis.
                        </p>
                    </section>
                </div>

                <!-- MCWC -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/MCWC" target="_blank" class="image featured">
                            <img src="images/mcwc.jpg" alt="MCWC" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/MCWC" target="_blank">MCWC</a></h3>
                        <p>
                            Multilingual constitutions from 191 nations, aligned for legal NLP and MT research.
                        </p>
                    </section>
                </div>

                <!-- Kalimat -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/KALIMAT" target="_blank" class="image featured">
                            <img src="images/kalimat.jpg" alt="Kalimat" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/KALIMAT" target="_blank">Kalimat</a></h3>
                        <p>
                            18,256 cleaned Arabic news articles in a modern ML-ready format.
                        </p>
                    </section>
                </div>

                <!-- Habibi -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/Habibi" target="_blank" class="image featured">
                            <img src="images/habibi.jpg" alt="Habibi" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/Habibi" target="_blank">Habibi</a></h3>
                        <p>
                            30,000+ Arabic song lyrics across 18 countries for dialect, authorship, and cultural NLP.
                        </p>
                    </section>
                </div>

                <!-- EASC -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/EASC" target="_blank" class="image featured">
                            <img src="images/EASC.jpg" alt="EASC" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/EASC" target="_blank">EASC</a></h3>
                        <p>
                            153 documents and 765 human-made summaries. A foundational resource for Arabic extractive summarisation.
                        </p>
                    </section>
                </div>

                <!-- MultiLing -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/Multilingual-Summarization" target="_blank" class="image featured">
                            <img src="images/multiling.jpg" alt="MultiLing" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/Multilingual-Summarization" target="_blank">MultiLing</a></h3>
                        <p>
                            A consolidated multilingual summarisation benchmark from MultiLing 2011–2013, cleaned and modernised.
                        </p>
                    </section>
                </div>

                <!-- Arabic Dialects -->
                <div class="col-3 col-6-medium col-12-small">
                    <section class="box feature">
                        <a href="https://huggingface.co/datasets/drelhaj/Arabic-Dialects" target="_blank" class="image featured">
                            <img src="images/arabic-dialects.jpg" alt="Arabic Dialects" class="feature-img" />
                        </a>
                        <h3><a href="https://huggingface.co/datasets/drelhaj/Arabic-Dialects" target="_blank">Arabic Dialects</a></h3>
                        <p>
                            A bivalency and code-switching focused corpus covering five major Arabic dialect varieties.
                        </p>
                    </section>
                </div>

                <!-- Buttons -->
                <div class="col-12">
                    <ul class="actions">
                        <li><a href="https://huggingface.co/drelhaj" class="button large" target="_blank">View All Datasets</a></li>
                        <li><a href="https://github.com/ArabicNLP-UK" class="button alt large" target="_blank">GitHub Organisation</a></li>
                    </ul>
                </div>

            </div>
        </div>
    </section>

</div>
<br>
<br>
<h2 class="major"><span>Featured Corpus</span></h2>

<div class="row">

    <!-- LEFT COLUMN: List of corpora -->
    <div class="col-3 col-12-small">
        <section class="box">
            <h4>Browse Corpora</h4>
            <ul class="links" id="corpus-links">
                <!-- JS will fill this automatically -->
            </ul>
        </section>
    </div>

    <!-- RIGHT COLUMN: Featured Corpus -->
    <div class="col-9 col-12-small">
        <article class="box post" id="featured-corpus">
            <header>
                <h3><a id="fc-title" href="#" target="_blank"></a></h3>
                <p id="fc-subtitle"></p>
            </header>

            <a id="fc-link" href="#" target="_blank" class="image featured">
                <img id="fc-image" src="images/pic05.jpg" alt="Corpus Image" />
            </a>

            <p id="fc-desc"></p>

            <a id="fc-button" href="#" target="_blank" class="button">View Dataset</a>
        </article>
    </div>

</div>
<br>
<br>

<section>
    <h2 class="major"><span>All Corpora</span></h2>
    <div id="corpora-container">Loading...</div>
</section>

							</div>
						</div>
					</div>
				</section>

			<!-- Footer -->
				<footer id="footer">
					<div class="container">
						<div class="row gtr-200">
							<div class="col-12">

								<!-- About -->
								<section>
									<h2 class="major"><span>About</span></h2>
									<p>
										This site showcases the ArabicNLP-UK collection of corpora, tools, and datasets created and maintained by 
										<strong><a href="https://elhaj.uk" target="_blank">Dr Mo El-Haj</a></strong>. The resources support work in Arabic NLP, low-resource languages, 
										multilingual processing, and specialised domains such as finance, legal text, and education.
										All datasets are hosted on GitHub and HuggingFace to ensure stable access, long-term preservation, 
										and easy integration into modern NLP workflows.
									</p>
								</section>


							</div>
							<div class="col-12">

								<!-- Contact -->
								<section>
									<h2 class="major"><span>Get in touch</span></h2>

									<p>
										<strong>Dr Mo El-Haj</strong><br>
										Director of NLP, VinUniversity, Vietnam<br>
										Lancaster University, UK
									</p>

									<p>
										<a href="mailto:dr.melhaj@gmail.com">dr.melhaj@gmail.com</a><br>
										<a href="https://elhaj.uk" target="_blank">elhaj.uk</a><br>
										<a href="https://vinnlp.com" target="_blank">vinnlp.com</a><br>
										<a href="https://arabicnlp.uk" target="_blank">arabicnlp.uk</a>
									</p>

										<ul class="contact">

											<!-- LinkedIn -->
											<li>
												<a class="icon brands fa-linkedin-in" 
												   href="https://www.linkedin.com/in/melhaj/" 
												   target="_blank">
												   <span class="label">LinkedIn</span>
												</a>
											</li>

											<!-- GitHub -->
											<li>
												<a class="icon brands fa-github" 
												   href="https://github.com/drelhaj" 
												   target="_blank">
												   <span class="label">GitHub</span>
												</a>
											</li>



										</ul>

								</section>


							</div>
						</div>

						<!-- Copyright -->
						<div id="copyright">
							<ul class="menu">
								<li>&copy; ArabicNLP-UK. All rights reserved.</li>
								<li>Design adapted from <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
							</ul>
						</div>


					</div>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

<script>
    const corpora = [
        {
            id: "arafinnews",
            title: "AraFinNews",
            subtitle: "The Arabic Financial News Dataset (212K)",
            image: "images/arafinnews.jpg",
            link: "https://huggingface.co/datasets/drelhaj/AraFinNews",
            longHtml: `
                <p><strong>AraFinNews</strong> is the largest openly available dataset of Arabic financial news, 
                comprising <strong>212,500 full-length articles</strong> collected from 
                <a href="https://www.argaam.com/" target="_blank" rel="noopener">Argaam.com</a>, 
                a leading financial news portal in the Arab world. The dataset provides structured, 
                machine-readable text suitable for research in <strong>financial NLP</strong>, 
                <strong>abstractive summarisation</strong>, <strong>event extraction</strong>, and 
                <strong>domain-specific language modelling</strong>.</p>

                <p>Comparable in spirit to the <strong>CNN/DailyMail</strong> dataset for English, 
                AraFinNews offers an Arabic counterpart for headline-style abstractive summarisation. 
                Each record pairs a full Arabic financial article with its professionally written headline, 
                enabling high-quality training and evaluation of summarisation and financial text understanding systems.</p>

                <h4>Dataset Overview</h4>
                <ul>
                    <li><strong>Total articles:</strong> 212,500</li>
                    <li><strong>Language:</strong> Modern Standard Arabic</li>
                    <li><strong>Domain:</strong> Finance, markets, economics, corporate activity</li>
                    <li><strong>Format:</strong> CSV (UTF-8), JSON (see GitHub)</li>
                    <li><strong>Licence:</strong> CC BY-NC 4.0</li>
                </ul>
                <p>Core fields include:
                    <ul>
                        <li><code>id</code> – unique numeric identifier</li>
                        <li><code>title</code> – Arabic headline</li>
                        <li><code>date</code> – publication date (ISO)</li>
                        <li><code>article</code> – full article text</li>
                        <li><code>url</code> – original Argaam.com link</li>
                    </ul>
                </p>

                <h4>Data Splits</h4>
                <p>The dataset provides ready-to-use CSV splits:</p>
                <ul>
                    <li><code>AraFinNews_train.csv</code> – 80% of the data</li>
                    <li><code>AraFinNews_validation.csv</code> – 10%</li>
                    <li><code>AraFinNews_test.csv</code> – 10%</li>
                </ul>
                <p>Each split contains complete article–headline pairs and is directly compatible with the 
                Hugging Face Dataset Viewer. The original ID-only split files 
                (<code>..._train_ids.csv</code>, <code>..._val_ids.csv</code>, <code>..._test_ids.csv</code>) 
                are retained for users who prefer to work from the master <code>AraFinNews.csv</code>.</p>

                <h4>Intended Use</h4>
                <p>AraFinNews supports research on:</p>
                <ul>
                    <li>Abstractive and extractive summarisation of financial news</li>
                    <li>Event and entity extraction in financial narratives</li>
                    <li>Sentiment and stance analysis for markets and corporate reporting</li>
                    <li>Domain-specific pretraining and adaptation of Arabic LLMs</li>
                    <li>Financial question answering and narrative analysis</li>
                </ul>
                <p>The dataset is released for <strong>non-commercial research and educational use</strong>.</p>

                <h4>Access & Code</h4>
                <p>For JSON files, parsing scripts, and full repository structure, see the 
                <a href="https://github.com/ArabicNLP-UK/AraFinNews" target="_blank" rel="noopener">
                AraFinNews GitHub repo</a>.</p>

                <h4>Citation</h4>
                <p>If you use AraFinNews, please cite:</p>
                <p>
                    <em>El-Haj, M. &amp; Rayson, P. (2025). AraFinNews: Arabic Financial Summarisation with 
                    Domain-Adapted LLMs. Proceedings of IEEE Big Data 2025.</em>
                </p>
                <p>Preprint: 
                    <a href="https://arxiv.org/abs/2511.01265" target="_blank" rel="noopener">
                    https://arxiv.org/abs/2511.01265</a>
                </p>

                <h4>Contact</h4>
                <p>
                    <strong>Mo El-Haj</strong><br>
                    VinUniversity, Hanoi / Lancaster University, UK<br>
                    <a href="mailto:dr.melhaj@gmail.com">dr.melhaj@gmail.com</a>
                </p>
            `
        },

        // The rest can stay as before (shorter descriptions for now)
		{
			id: "arabjobs",
			title: "ArabJobs",
			subtitle: "A Multinational Corpus of Arabic Job Advertisements",
			image: "images/arabjobs.jpg",
			link: "https://huggingface.co/datasets/drelhaj/ArabJobs",
			longHtml: `
				<p><strong>ArabJobs</strong> is the first publicly available, multinational corpus of 
				<strong>Arabic job advertisements</strong>, collected from Egypt, Jordan, Saudi Arabia, and the UAE. 
				It contains <strong>8,546 job postings</strong> and more than <strong>550,000 words</strong>, with 
				coverage across a wide range of professions, sectors, and dialects.</p>

				<p>The dataset includes rich metadata such as gender indicators, profession labels, hierarchical 
				job categories, salary fields, and linguistic markers related to bivalency and dialectal 
				variation. It supports research on:</p>

				<ul>
					<li>Gender representation and bias in recruitment language</li>
					<li>Dialectal variation and written code-switching</li>
					<li>Salary estimation and profession classification</li>
					<li>Fairness-aware Arabic NLP</li>
					<li>Labour market discourse across Arab countries</li>
				</ul>

				<h4>Dataset Structure</h4>
				<p>Each row represents a single job advertisement with fields including:</p>
				<ul>
					<li><code>job_title</code> — job title in Arabic</li>
					<li><code>description</code> — full job description text</li>
					<li><code>country</code> — posting country</li>
					<li><code>location</code> — city or region</li>
					<li><code>salary</code> — raw salary text</li>
					<li><code>salary_local</code>, <code>salary_usd</code> — normalised salary values</li>
					<li><code>profession</code> — derived profession label</li>
					<li><code>gender</code> — male, female, or neutral (when detectable)</li>
					<li><code>job_category</code>, <code>sub_category</code> — hierarchical category labels</li>
				</ul>

				<h4>Citation</h4>
				<p>If you use ArabJobs, please cite:</p>
				<a href="https://arxiv.org/pdf/2509.22589" target="_blank">
				<p><em>El-Haj, M. (2025). ArabJobs: A Multinational Corpus of Arabic Job Ads. 
				Proceedings of the ArabicNLP Workshop, EMNLP, Suzhou, China.</em></p></a>

				<pre><code>@inproceedings{elhaj2025arabjobs,
		  title={ArabJobs: A Multinational Corpus of Arabic Job Ads},
		  author={El-Haj, Mo},
		  booktitle={Proceedings of the ArabicNLP Workshop, EMNLP},
		  year={2025},
		  address={Suzhou, China},
		  month={November}
		}</code></pre>

				<h4>Visualisations</h4>
				<p>Exploratory word clouds, country distributions, and dialect usage maps are provided in the GitHub repository.</p>

				<div style="text-align:center;">
					<img src="https://raw.githubusercontent.com/drelhaj/ArabJobs/main/figures/professionwordcloud_male.png" width="260" style="margin:5px;">
					<img src="https://raw.githubusercontent.com/drelhaj/ArabJobs/main/figures/professionwordcloud_female.png" width="260" style="margin:5px;">
					<br>
					<img src="https://raw.githubusercontent.com/drelhaj/ArabJobs/main/figures/ads_by_country_overall_english.png" width="500" style="margin-top:10px;">
					<br>
					<img src="https://raw.githubusercontent.com/drelhaj/ArabJobs/main/figures/dialects-english.png" width="550" style="margin-top:10px;">
				</div>

				<h4>License</h4>
				<p>The dataset is licensed under <strong>CC BY 4.0</strong>, permitting research and academic reuse with attribution.</p>

				<h4>Repository</h4>
				<p>GitHub: 
					<a href="https://github.com/drelhaj/ArabJobs" target="_blank">
						https://github.com/drelhaj/ArabJobs
					</a>
				</p>
			`
		},
		{
			id: "mcwc",
			title: "MCWC",
			subtitle: "The Multilingual Corpus of World’s Constitutions",
			image: "images/mcwc.jpg",
			link: "https://huggingface.co/datasets/drelhaj/MCWC",
			longHtml: `
				<p>The <strong>Multilingual Corpus of World’s Constitutions (MCWC)</strong> is a curated,
				multilingual collection of constitutional texts from <strong>191 countries</strong>, including
				both current and historical versions. It provides aligned constitutional material in
				<strong>English, Arabic, and Spanish</strong>, enabling comparative legal analysis,
				multilingual NLP research, and high-quality machine translation experiments.</p>

				<p>The CSV release contains a cleaned, normalised, and sentence-aligned version of the corpus,
				suitable for work in legal NLP, cross-lingual retrieval, topic modelling, and
				multilingual language modelling.</p>

				<p><strong>Paper (OSACT @ LREC-COLING 2024):</strong><br>
				<a href="https://aclanthology.org/2024.osact-1.7/" target="_blank">
					https://aclanthology.org/2024.osact-1.7/
				</a></p>

				<h4>Contents of the Dataset</h4>
				<p><code>MCWC.csv</code> includes:</p>
				<ul>
					<li>Constitutional text in <strong>three languages</strong>: English, Arabic, Spanish</li>
					<li><strong>Pairwise sentence alignment</strong> via a stable <code>align_id</code></li>
					<li><strong>Country</strong> and <strong>continent</strong> metadata</li>
					<li>One row per sentence for easy filtering and cross-lingual comparison</li>
					<li><strong>Machine-translated</strong> texts for constitutions lacking Arabic or Spanish versions</li>
					<li>Cleaned and normalised content ready for NLP pipelines</li>
				</ul>

				<p>Typical columns include:</p>
				<ul>
					<li><code>country</code></li>
					<li><code>continent</code></li>
					<li><code>align_id</code></li>
					<li><code>text_en</code>, <code>text_ar</code>, <code>text_es</code></li>
					<li><code>constitution_year</code></li>
					<li><code>language_source</code> (original vs. machine-translated)</li>
				</ul>

				<h4>Motivation</h4>
				<p>Constitutions are foundational legal documents, yet multilingual access is fragmented across
				formats and repositories. MCWC resolves these challenges by offering a unified, aligned,
				multilingual corpus tailored for:</p>

				<ul>
					<li>comparative constitutional research</li>
					<li>legal information retrieval</li>
					<li>multilingual and legal machine translation</li>
					<li>cross-lingual semantic similarity</li>
					<li>topic modelling and corpus linguistics</li>
				</ul>

				<p>The acronym <strong>MCWC</strong> is pronounced “<em>Makkuk</em>” — an Arabic word for
				<em>space shuttle</em> — symbolising cross-lingual travel between legal systems.</p>

				<h4>Data Sources & Preparation</h4>
				<p>The corpus integrates material from the Comparative Constitutions Project, the Constitute
				Project, Wikipedia, and governmental archives. Raw XML files were cleaned, segmented, and
				aligned using structural cues such as article numbers. When Arabic or Spanish versions were
				unavailable, translations were generated using a fine-tuned MT model.</p>

				<p>Additional processing steps included:</p>
				<ul>
					<li>continent classification</li>
					<li>tokenisation and normalisation</li>
					<li>vocabulary overlap and similarity analysis</li>
					<li>TF-IDF cosine similarity (legal English)</li>
				</ul>

				<h4>Dataset Statistics</h4>
				<ul>
					<li><strong>223 constitutions</strong></li>
					<li><strong>191 countries</strong></li>
					<li><strong>95</strong> constitutions available in all three languages</li>
					<li><strong>52,177</strong> aligned English–Arabic sentences</li>
					<li><strong>48,892</strong> aligned English–Spanish sentences</li>
					<li><strong>27,352</strong> aligned Arabic–Spanish sentences</li>
					<li><strong>236,156</strong> machine-generated translations (SeamlessM4T-v2)</li>
				</ul>

				<p>By continent:</p>
				<ul>
					<li>Africa: 65</li>
					<li>Asia: 54</li>
					<li>Europe: 49</li>
					<li>North America: 26</li>
					<li>South America: 15</li>
					<li>Oceania: 14</li>
				</ul>

				<h4>Translation & Evaluation</h4>
				<p>Constitutions missing Arabic or Spanish texts were translated using
				<strong>Seamless-m4t-v2-large</strong>. Translation performance:</p>

				<ul>
					<li><strong>BLEU = 0.68</strong> on 500 manually inspected En–Ar / En–Es pairs</li>
					<li>Human evaluation by two Arabic NLP experts</li>
					<li><strong>Cohen’s κ = 0.30</strong> (label imbalance)</li>
					<li><strong>Krippendorff’s α ≈ 0.90</strong> (strong reliability)</li>
				</ul>

				<p>Fine-tuned Marian NMT models trained on the corpus improved translation quality across all
				six language pairs — these models are available on HuggingFace.</p>

				<h4>Train / Validation / Test Splits</h4>
				<p>The original paper does not specify official splits, so we provide a recommended 
				<strong>80/10/10</strong> split using a fixed seed. This ensures shuffled and reproducible
				partitioning over <code>MCWC.csv</code>.</p>

				<h4>Ethical Considerations</h4>
				<ul>
					<li>Corpus distributes aligned and processed text, not the original XML sources</li>
					<li>Some legal phrasing may not translate literally</li>
					<li>Historical constitutions reflect ideology of their era</li>
				</ul>

				<h4>Citation</h4>
				<p><strong>El-Haj, M. & Ezzini, S. (2024).</strong><br>
				<em>The Multilingual Corpus of World’s Constitutions (MCWC).</em><br>
				OSACT Workshop @ LREC-COLING 2024.</p>

				<p><a href="https://aclanthology.org/2024.osact-1.7/" target="_blank">
					https://aclanthology.org/2024.osact-1.7/
				</a></p>

				<h4>Contact</h4>
				<p><strong>Mo El-Haj</strong><br>
				UCREL NLP Group<br>
				Lancaster University & VinUniversity<br>
				Email: m.el-haj@lancaster.ac.uk / elhaj.m@vinuni.edu.vn</p>
			`
		},
		{
			id: "habibi",
			title: "Habibi",
			subtitle: "A Multi-Dialect, Multi-National Arabic Song Lyrics Corpus",
			image: "images/habibi.jpg",
			link: "https://huggingface.co/datasets/drelhaj/Habibi",
			longHtml: `
				<p><strong>Habibi</strong> is the first large-scale open corpus of Arabic song lyrics, 
				comprising more than <strong>30,000 songs</strong> performed by artists from 
				<strong>18 Arab countries</strong>. The collection spans <strong>six major Arabic dialects</strong>,
				segmented into over <strong>520,000 verses</strong> and totalling more than 
				<strong>3.5 million words</strong>.</p>

				<p>The corpus was created to support computational research in 
				<strong>Arabic dialect identification</strong>, 
				<strong>country-of-origin classification</strong>, 
				<strong>authorship attribution</strong>, and broader studies in 
				<strong>Arabic language variation</strong> within contemporary music.</p>

				<h4>Corpus Overview</h4>
				<p>Each song record includes:</p>
				<ul>
					<li>Singer name</li>
					<li>Song title</li>
					<li>Country of origin</li>
					<li>Dialect category</li>
					<li>Writer and composer (when available)</li>
					<li>Lyrics segmented into individual verses</li>
				</ul>

				<p>Lyrics were collected using a Web-as-Corpus approach and manually curated to produce
				a clean dataset free of emojis, hashtags, duplicated content, and social-media noise.</p>

				<h4>Key Statistics</h4>
				<ul>
					<li><strong>30,072</strong> songs</li>
					<li><strong>527,870</strong> verses</li>
					<li><strong>3.57 million</strong> words</li>
					<li><strong>1,765</strong> singers</li>
					<li><strong>3,789</strong> writers</li>
					<li><strong>2,463</strong> composers</li>
					<li><strong>18</strong> countries</li>
					<li><strong>6</strong> dialect families</li>
				</ul>

				<h4>Dialect Coverage</h4>
				<p>Songs are grouped into six widely recognised dialect categories:</p>
				<ul>
					<li>Egyptian</li>
					<li>Gulf</li>
					<li>Levantine</li>
					<li>Iraqi</li>
					<li>Sudanese</li>
					<li>Maghrebi</li>
				</ul>

				<p>Dialect assignment follows conventions used by major Arabic music platforms such as
				Anghami, Spotify, and Deezer — based on the singer’s country of origin rather than
				lyrical content.</p>

				<h4>File Formats</h4>
				<p>The corpus is distributed in multiple machine-readable formats:</p>
				<ul>
					<li><strong>CSV</strong> (primary format) with full metadata and verse-level segmentation</li>
					<li><strong>Annotated TXT</strong> using lightweight XML-style tags</li>
					<li><strong>JSON</strong> and <strong>XML</strong> versions derived from the master CSV</li>
				</ul>

				<h4>Recommended Use-Cases</h4>
				<p>Habibi supports research in:</p>
				<ul>
					<li>Dialect identification (verse, song, or artist level)</li>
					<li>Country-of-origin classification</li>
					<li>Authorship attribution (singer, writer, composer)</li>
					<li>Linguistic variation across the Arab world</li>
					<li>Domain-specific embeddings training</li>
					<li>Sociolinguistic and stylistic analysis</li>
				</ul>

				<h4>Data Splits</h4>
				<p>The dataset includes standard train/validation/test splits (80/10/10) generated from 
				the master <code>habibi.csv</code> file using a fixed random seed.</p>

				<pre><code>import pandas as pd
		from sklearn.model_selection import train_test_split

		df = pd.read_csv("habibi.csv")

		train, temp = train_test_split(df, test_size=0.2, random_state=42)
		val, test = train_test_split(temp, test_size=0.5, random_state=42)

		train.to_csv("train.csv", index=False)
		val.to_csv("validation.csv", index=False)
		test.to_csv("test.csv", index=False)</code></pre>

				<h4>Benchmarks</h4>
				<p>Experiments were conducted on:</p>
				<ul>
					<li>Binary and multi-class dialect classification</li>
					<li>Binary and multi-class country-of-origin classification</li>
				</ul>
				
				<p>Models evaluated include:</p>
				<ul>
					<li>Naïve Bayes, Logistic Regression, SVM</li>
					<li>CNN, LSTM, BiLSTM, CLSTM, BiGRU</li>
					<li>FastText embeddings (Arabic)</li>
					<li>In-house <strong>Habibi CBOW embeddings</strong> (300-dimensional)</li>
				</ul>

				<p><strong>CNN (word-level)</strong> delivered the strongest deep-learning performance, while 
				<strong>Naïve Bayes</strong> consistently outperformed other classical models in multi-class tasks.</p>

				<h4>Downloads & Resources</h4>
				<p>All formats (CSV, TXT, JSON, XML) and the <strong>Habibi word embeddings</strong> are available:</p>
				<p><a href="http://ucrel-web.lancaster.ac.uk/habibi/" target="_blank">
					http://ucrel-web.lancaster.ac.uk/habibi/
				</a></p>

				<h4>Citation</h4>
				<p><strong>El-Haj, M. (2020).</strong><br>
				<em>Habibi – a multi-dialect multi-national Arabic song lyrics corpus.</em><br>
				Proceedings of LREC 2020, pp. 1318–1326.</p>

				<pre><code>@inproceedings{elhaj2020habibi,
		  title={Habibi--a multi dialect multi national Arabic song lyrics corpus},
		  author={El-Haj, Mahmoud},
		  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},
		  pages={1318--1326},
		  year={2020}
		}</code></pre>
			`
		},
		{
			id: "kalimat",
			title: "Kalimat",
			subtitle: "A Multipurpose Arabic News Corpus",
			image: "images/kalimat.jpg",
			link: "https://huggingface.co/datasets/drelhaj/KALIMAT",
			longHtml: `
				<p><strong>Kalimat</strong> is a cleaned and consolidated version of the original 
				<em>Kalimat – a multipurpose Arabic Corpus</em>, containing 
				<strong>18,256 Arabic news articles</strong> sourced from a wide range of domains. 
				Originally distributed as thousands of individual <code>.txt</code> files, this edition 
				reconstructs and normalises all documents into modern, machine-learning-friendly formats.</p>

				<h4>Corpus Overview</h4>
				<p>The collection covers more than <strong>20,000</strong> articles across several key categories:</p>
				<ul>
					<li>Politics</li>
					<li>Economy</li>
					<li>Culture</li>
					<li>Religion</li>
					<li>Sport</li>
					<li>Social topics</li>
					<li>Other sub-domains derived from folder structure</li>
				</ul>

				<p>The original corpus stored articles as <em>one word per line</em>. 
				This cleaned edition:</p>
				<ul>
					<li>reconstructs natural text with proper spacing</li>
					<li>ensures consistent UTF-8 encoding</li>
					<li>extracts metadata (category, filename, date, id)</li>
					<li>provides unified CSV + JSONL formats</li>
				</ul>

				<h4>Metadata Fields</h4>
				<p>Each document includes:</p>
				<ul>
					<li><strong>id</strong> — numeric identifier from filename (or <code>-1</code> if missing)</li>
					<li><strong>filename</strong> — original source filename</li>
					<li><strong>category</strong> — derived from folder structure</li>
					<li><strong>year_month</strong> — extracted from filename, else <code>"unknown"</code></li>
					<li><strong>text</strong> — reconstructed article text</li>
				</ul>

				<h4>Provided Formats</h4>
				<ul>
					<li><strong>CSV</strong> — <code>kalimat.csv</code> containing all articles and metadata</li>
					<li><strong>JSONL</strong> — <code>kalimat.jsonl</code> for modern NLP training</li>
					<li><strong>TXT (zipped)</strong> — original filenames preserved, reconstructed per article</li>
				</ul>

				<h4>Train / Validation / Test Splits</h4>
				<p>A standard <strong>80/10/10</strong> split is included:</p>
				<ul>
					<li><code>kalimat_train.csv</code></li>
					<li><code>kalimat_val.csv</code></li>
					<li><code>kalimat_test.csv</code></li>
				</ul>

				<pre><code>import pandas as pd
		from sklearn.model_selection import train_test_split

		df = pd.read_csv("kalimat.csv", encoding="utf-8")

		train_df, temp_df = train_test_split(
			df, test_size=0.20, random_state=42, shuffle=True
		)

		val_df, test_df = train_test_split(
			temp_df, test_size=0.50, random_state=42, shuffle=True
		)

		train_df.to_csv("kalimat_train.csv", index=False, encoding="utf-8")
		val_df.to_csv("kalimat_val.csv", index=False, encoding="utf-8")
		test_df.to_csv("kalimat_test.csv", index=False, encoding="utf-8")</code></pre>

				<h4>Repository Structure</h4>
				<pre><code>kalimat.csv
		kalimat.jsonl
		kalimat_train.csv
		kalimat_val.csv
		kalimat_test.csv
		kalimat_txt.zip
		README.md</code></pre>

				<h4>Citation</h4>
				<p>If you use Kalimat, please cite the original paper:</p>
				<p><strong>El-Haj, M. & Koulali, R. (2013).</strong><br>
				<em>Kalimat: a multipurpose Arabic corpus.</em><br>
				Second Workshop on Arabic Corpus Linguistics (WACL-2), pp. 22–25.</p>

				<p><a href="https://elhaj.uk/docs/KALIMAT_ELHAJ_KOULALI.pdf" target="_blank">
					https://elhaj.uk/docs/KALIMAT_ELHAJ_KOULALI.pdf
				</a></p>
			`
		},

		{
			id: "easc",
			title: "EASC",
			subtitle: "The Essex Arabic Summaries Corpus",
			image: "images/EASC.jpg",
			link: "https://huggingface.co/datasets/drelhaj/EASC",
			longHtml: `
				<p><strong>EASC</strong> is a collection of <strong>153 Arabic source documents</strong> and 
				<strong>765 human-generated extractive summaries</strong>, created via Amazon Mechanical Turk. 
				It is one of the earliest and most widely cited datasets for 
				<strong>Arabic single-document extractive summarisation</strong>.</p>

				<h4>Background</h4>
				<p>The dataset was introduced in:</p>
				<p><strong>El-Haj, M., Kruschwitz, U., & Fox, C. (2010).</strong><br>
				<em>Using Mechanical Turk to Create a Corpus of Arabic Summaries.</em><br>
				Workshop on LRs & HLT for Semitic Languages @ LREC 2010.</p>

				<p>Each article has <strong>five independent extractive summaries</strong>, allowing for 
				multi-reference evaluation and sentence-importance modelling. 
				Follow-up expansions include the 2011 AIRS paper and the 2012 PhD thesis on Arabic multi-document summarisation.</p>

				<h4>Corpus Contents</h4>
				<table>
					<tr><th>Component</th><th>Count</th><th>Description</th></tr>
					<tr><td>Articles</td><td>153</td><td>Wikipedia + AlRai (Jordan) + AlWatan (Saudi Arabia)</td></tr>
					<tr><td>Summaries</td><td>765</td><td>Five extractive MTurk summaries per article</td></tr>
					<tr><td>Topics</td><td>10</td><td>Covering politics, sport, religion, economics, health, etc.</td></tr>
				</table>

				<p>Summarisers were instructed to select up to <strong>50% of sentences</strong> they judged most important.</p>

				<h4>Original Directory Structure</h4>
				<pre><code>Articles/
		  Article001/
		  Article002/
		  ...
		MTurk/
		  Article001/
		  Article002/
		  ...</code></pre>

				<p>
					<code>Articles/</code> contains full documents,  
					<code>MTurk/</code> contains five summaries per article (<code>A–E</code>).
				</p>

				<h4>Modern CSV / JSONL Format</h4>
				<p>The repository provides unified versions suitable for modern NLP workflows.</p>

				<h5>CSV Schema</h5>
				<ul>
					<li><strong>article_id</strong></li>
					<li><strong>topic_name</strong></li>
					<li><strong>article_text</strong></li>
					<li><strong>summary_A</strong> ... <strong>summary_E</strong></li>
				</ul>

				<h5>JSONL Schema</h5>
				<pre><code>{
		  "article_id": 1,
		  "topic_name": "Art and Music",
		  "article_text": "...",
		  "summaries": ["...", "...", "...", "...", "..."]
		}</code></pre>

				<h4>Train / Validation / Test Splits</h4>
				<p>Standard <strong>80/10/10</strong> splits included:</p>
				<ul>
					<li><code>EASC_train.csv</code></li>
					<li><code>EASC_val.csv</code></li>
					<li><code>EASC_test.csv</code></li>
				</ul>

				<h4>Recommended Gold Standards</h4>
				<p>Based on the LREC 2010 paper:</p>
				<ul>
					<li><strong>Level 3</strong> — sentences chosen by ≥3 workers (recommended)</li>
					<li><strong>Level 2</strong> — sentences chosen by ≥2 workers</li>
					<li><strong>All</strong> — all selected sentences (not recommended for evaluation)</li>
				</ul>

				<h4>Intended Use</h4>
				<p>EASC is suited for research in:</p>
				<ul>
					<li>Extractive summarisation</li>
					<li>Sentence scoring and ranking</li>
					<li>Human–machine evaluation</li>
					<li>Probabilistic sentence selection</li>
					<li>Crowdsourcing quality analysis</li>
				</ul>

				<h4>Evaluations</h4>
				<p>Systems evaluated on EASC include Sakhr Summariser, Gen-Summ, LSA-Summ, AQBTSS, and baseline extractive systems.</p>
				<p>Metrics:</p>
				<ul>
					<li><strong>Dice coefficient</strong> (recommended)</li>
					<li>ROUGE-2 / ROUGE-L / ROUGE-W / ROUGE-S</li>
					<li>AutoSummENG</li>
				</ul>

				<h4>Citation</h4>
				<p><strong>El-Haj, M., Kruschwitz, U., & Fox, C. (2010).</strong><br>
				<em>Using Mechanical Turk to Create a Corpus of Arabic Summaries.</em><br>
				Workshop on LRs & HLT for Semitic Languages @ LREC 2010.</p>

				<p>Additional references:</p>
				<ul>
					<li>El-Haj (2012). <em>Multi-document Arabic Text Summarisation.</em> PhD Thesis.</li>
					<li>El-Haj, Kruschwitz & Fox (2011). AIRS 2011.</li>
				</ul>

				<h4>Licence</h4>
				<p>The cleaned version follows the original academic research usage terms.</p>

				<h4>Maintainer</h4>
				<p><strong>Dr Mo El-Haj</strong><br>
				Associate Professor in NLP<br>
				VinUniversity, Vietnam / Lancaster University, UK</p>
			`
		},
		{
			id: "multiling",
			title: "MultiLing",
			subtitle: "Multilingual Single & Multi-Document Summarisation Corpus",
			image: "images/multiling.jpg",
			link: "https://huggingface.co/datasets/drelhaj/Multilingual-Summarization",
			longHtml: `
				<p>
					The <strong>MultiLing Multilingual Summarisation Corpus</strong> is a large multilingual
					benchmark covering <strong>single-document</strong> and 
					<strong>multi-document abstractive summarisation</strong>, created for the
					MultiLing 2011 and 2013 shared tasks under ACL.
				</p>

				<p>
					This release consolidates, cleans, and reformats the original resources into
					machine-readable formats suitable for sequence-to-sequence models and LLMs.
				</p>

				<h4>Languages</h4>
				<p>The dataset includes the ten principal languages of MultiLing 2013:</p>
				<ul>
					<li>Arabic</li>
					<li>Chinese</li>
					<li>Czech</li>
					<li>English</li>
					<li>French</li>
					<li>Greek</li>
					<li>Hebrew</li>
					<li>Hindi</li>
					<li>Romanian</li>
					<li>Spanish</li>
				</ul>

				<h4>Single-Document Summarisation</h4>
				<p>For each language:</p>
				<ul>
					<li>One WikiNews source article</li>
					<li>One human abstractive summary</li>
					<li>Fully parallel across languages</li>
				</ul>

				<h4>Multi-Document Summarisation</h4>
				<p>Each topic contains <strong>10 related news articles</strong> describing the same event sequence, along with:</p>
				<ul>
					<li>Three independent human-written abstractive summaries</li>
					<li>Canonical cluster IDs (e.g., M000, M014, M103)</li>
					<li>Word limit of ~240–250 words for summaries</li>
				</ul>

				<h4>Purpose and Design</h4>
				<p>The corpus was designed to support:</p>
				<ul>
					<li>Cross-lingual summarisation</li>
					<li>Multilingual evaluation of abstractive systems</li>
					<li>LLM benchmarking across many languages</li>
					<li>Multi-document sequence modelling</li>
					<li>Comparative difficulty analysis across languages</li>
				</ul>

				<h4>Source and Citation</h4>
				<p>
					<strong>Li L, Forăscu C, El-Haj M, Giannakopoulos G. (2013)</strong><br>
					<em>Multi-Document Multilingual Summarization Corpus Preparation, Part 1: Arabic, English, Greek, Chinese, Romanian.</em><br>
					Proceedings of the MultiLing 2013 Workshop, ACL 2013.<br>
					<a href="https://aclanthology.org/W13-3101.pdf" target="_blank">ACL Anthology PDF</a>
				</p>

				<h4>Dataset Structure</h4>

				<h5>Single-Document Format</h5>
				<ul>
					<li><strong>language</strong> — ISO code (ar, en, fr, ...)</li>
					<li><strong>doc_id</strong></li>
					<li><strong>document_text</strong></li>
					<li><strong>summary</strong></li>
				</ul>

				<h5>Multi-Document Format</h5>
				<ul>
					<li><strong>cluster_id</strong></li>
					<li><strong>language</strong></li>
					<li><strong>doc_ids</strong> — list of the 10 related documents</li>
					<li><strong>documents_text</strong> — concatenated with &lt;DOC id=...&gt; tags</li>
					<li><strong>summary_1</strong>, <strong>summary_2</strong>, <strong>summary_3</strong></li>
				</ul>

				<h4>Splits</h4>
				<p>Both single-document and multi-document datasets include deterministic:</p>
				<ul>
					<li>train</li>
					<li>validation</li>
					<li>test</li>
				</ul>
				<p>Multi-document splits are <strong>cluster-based</strong> to avoid leakage.</p>

				<h4>Recommended Use Cases</h4>
				<ul>
					<li>Multilingual abstractive summarisation</li>
					<li>Cross-lingual LLM evaluation</li>
					<li>Multi-document sequence modelling</li>
					<li>Evaluation metric research (ROUGE, NPowER, AutoSummENG-MeMoG)</li>
					<li>Low-resource summarisation studies</li>
				</ul>

				<h4>Loading the Dataset</h4>
				<pre><code>from datasets import load_dataset

		ds = load_dataset("YOUR_DATASET_NAME", "multi")
		# or
		ds = load_dataset("YOUR_DATASET_NAME", "single")</code></pre>

				<h4>Licence</h4>
				<p>
					All texts originate from WikiNews under Creative Commons BY 2.5/3.0.
					This consolidated release is provided under <strong>CC-BY-4.0</strong>.
				</p>

				<h4>Acknowledgements</h4>
				<p>
					MultiLing was developed by an international consortium spanning more than ten institutions.
					This cleaned and reformatted edition is designed to support modern multilingual summarisation research.
				</p>
			`
		},
		{
			id: "arabic_dialects",
			title: "Arabic Dialects Dataset",
			subtitle: "Bivalency, Code-Switching & Dialect Identification",
			image: "images/arabic-dialects.jpg",
			link: "https://huggingface.co/datasets/drelhaj/Arabic-Dialects",
			longHtml: `
				<p>
					The <strong>Arabic Dialects Dataset</strong> is a specialised corpus for 
					<strong>automatic dialect identification</strong> with a focus on 
					<strong>bivalency</strong> and <strong>written code-switching</strong> 
					between major Arabic dialects and Modern Standard Arabic (MSA).
					It covers five varieties: Egyptian (EGY), Gulf (GLF), Levantine (LAV), 
					North African/Tunisian (NOR), and MSA.
				</p>

				<p>
					The dataset has been used to evaluate novel methods such as 
					<strong>Subtractive Bivalency Profiling (SBP)</strong>, with supervised models 
					achieving over <strong>76% accuracy</strong>.
				</p>

				<h4>Dialects Included</h4>
				<ul>
					<li><strong>EGY</strong> – Egyptian Arabic</li>
					<li><strong>GLF</strong> – Gulf Arabic</li>
					<li><strong>LAV</strong> – Levantine Arabic</li>
					<li><strong>NOR</strong> – North African / Tunisian Arabic</li>
					<li><strong>MSA</strong> – Modern Standard Arabic</li>
				</ul>

				<h4>Citation</h4>
				<p>
					<strong>El-Haj M., Rayson P., Aboelezz M. (2018)</strong><br>
					<em>Arabic Dialect Identification in the Context of Bivalency and Code-Switching.</em><br>
					LREC 2018, Miyazaki, Japan.<br>
					<a href="https://elhaj.uk/docs/237_Paper.pdf" target="_blank">PDF</a>
				</p>

				<h4>Dataset Structure</h4>
				<p>The dataset is organised into two major components:</p>

				<h5>1. Dialects Full Text</h5>
				<pre><code>Dialects Full Text/
		│── allEGY.txt
		│── allGLF.txt
		│── allLAV.txt
		│── allMSA.txt
		└── allNOR.txt</code></pre>
				<p>
					Each file contains raw sentence-level samples, one per line, ready for direct
					classification experiments.
				</p>

				<h5>2. Dialectal Frequency Lists</h5>
				<pre><code>Dialects Frequency Lists/
		├── Bivalency Removed/
		├── Dialects’ MSA/
		├── Tokens WITH Frequency Count/
		└── Tokens NO Frequency Count/</code></pre>
				<p>
					These support linguistic analysis and the SBP method, including 
					bivalency-removed lists, code-switched MSA lists, and token frequency files.
				</p>

				<h4>CSV Conversion</h4>
				<p>
					All text files were merged into a unified <code>arabic_dialects_full.csv</code> 
					containing:
				</p>
				<ul>
					<li><strong>sentence</strong> – text sample</li>
					<li><strong>dialect</strong> – EGY / GLF / LAV / MSA / NOR</li>
				</ul>

				<h4>Splits</h4>
				<p>
					Stratified train/dev/test splits (80/10/10) are provided:
				</p>
				<ul>
					<li><code>arabic_dialects_train.csv</code></li>
					<li><code>arabic_dialects_dev.csv</code></li>
					<li><code>arabic_dialects_test.csv</code></li>
				</ul>

				<h4>Intended Use</h4>
				<ul>
					<li>Dialect identification (five-way classification)</li>
					<li>Bivalency and code-switching analysis</li>
					<li>Feature engineering using SBP lists</li>
					<li>Sociolinguistic variation studies</li>
				</ul>

				<h4>Dataset Statistics</h4>
				<table>
					<tr><th>Dialect</th><th>Sentences</th><th>Words</th></tr>
					<tr><td>EGY</td><td>4,061</td><td>118,152</td></tr>
					<tr><td>GLF</td><td>2,546</td><td>65,752</td></tr>
					<tr><td>LAV</td><td>2,463</td><td>67,976</td></tr>
					<tr><td>MSA</td><td>3,731</td><td>49,985</td></tr>
					<tr><td>NOR</td><td>3,693</td><td>53,204</td></tr>
					<tr><td><strong>Total</strong></td><td><strong>16,494</strong></td><td><strong>355,069</strong></td></tr>
				</table>

				<h4>Example Usage</h4>
				<pre><code>from datasets import load_dataset

		ds = load_dataset("YOUR_REPO_NAME", "full_text")
		print(ds["train"][0])</code></pre>

				<h4>Licence</h4>
				<p>
					Released for research use only. Texts originate from publicly available sources 
					where redistribution for academic use is permitted.
				</p>

				<h4>Acknowledgements</h4>
				<p>
					Developed at UCREL, Lancaster University, as part of research into Arabic 
					dialect variation, bivalency, and automatic identification.
				</p>
			`
		}
    ];

    // Pick a random corpus
    const chosen = corpora[Math.floor(Math.random() * corpora.length)];

    // Fill in base fields
    document.getElementById("fc-title").textContent = chosen.title;
    document.getElementById("fc-title").href = chosen.link;

    document.getElementById("fc-subtitle").textContent = chosen.subtitle;

    document.getElementById("fc-image").src = chosen.image;
    document.getElementById("fc-image").alt = chosen.title;

    document.getElementById("fc-link").href = chosen.link;

    // Long HTML description if available, otherwise short text
    const descEl = document.getElementById("fc-desc");
    if (chosen.longHtml) {
        descEl.innerHTML = chosen.longHtml;
    } else {
        descEl.textContent = chosen.desc;
    }

    document.getElementById("fc-button").href = chosen.link;
</script>
<script>
const corpusLinks = document.getElementById("corpus-links");

// Fill sidebar list
corpora.forEach(corpus => {
    const li = document.createElement("li");
    const a = document.createElement("a");

    a.href = "#";
    a.textContent = corpus.title;

    a.onclick = (event) => {
        event.preventDefault();
        loadCorpus(corpus.id);
    };

    li.appendChild(a);
    corpusLinks.appendChild(li);
});

function loadCorpus(id) {
    const corpus = corpora.find(c => c.id === id);
    if (!corpus) return;

    // Title link
    const titleLink = document.getElementById("fc-title");
    titleLink.textContent = corpus.title;
    titleLink.href = corpus.link;
    titleLink.target = "_blank";

    // Subtitle
    document.getElementById("fc-subtitle").textContent = corpus.subtitle;

    // Image + its clickable link
    const img = document.getElementById("fc-image");
    img.src = corpus.image;
    img.alt = corpus.title;

    const imgLink = document.getElementById("fc-link");
    imgLink.href = corpus.link;
    imgLink.target = "_blank";

    // Description
    document.getElementById("fc-desc").innerHTML = corpus.longHtml;

    // Button link
    const button = document.getElementById("fc-button");
    button.href = corpus.link;
    button.target = "_blank";
}

// Load random corpus on page load
window.addEventListener("DOMContentLoaded", () => {
    const randomCorpus = corpora[Math.floor(Math.random() * corpora.length)];
    loadCorpus(randomCorpus.id);
});
</script>
<script>
fetch("https://elhaj.uk/corpora.html")
    .then(response => response.text())
    .then(html => {
        const parser = new DOMParser();
        const doc = parser.parseFromString(html, "text/html");

        // Extract only the main content, not header/footer
        const main = doc.querySelector("main") || doc.body;

        document.getElementById("corpora-container").innerHTML = main.innerHTML;
    })
    .catch(err => {
        document.getElementById("corpora-container").innerHTML =
            "Failed to load corpora list.";
    });
</script>


	</body>
</html>